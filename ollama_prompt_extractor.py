import os
import json
import requests

def get_available_models(base_url):
    try:
        response = requests.get(f"{base_url}/api/tags")
        if response.status_code == 200:
            models = response.json()['models']
            return [model['name'] for model in models]
        else:
            print(f"Error fetching models: {response.status_code}")
            return []
    except requests.RequestException as e:
        print(f"Error connecting to Ollama: {str(e)}")
        return []

class OllamaPromptExtractor:
    base_url = "http://localhost:11434"
    available_models = []

    @classmethod
    def initialize(cls):
        cls.load_config()
        cls.available_models = get_available_models(cls.base_url)

    @classmethod
    def load_config(cls):
        config_path = os.path.join(os.path.dirname(__file__), 'config.json')
        try:
            with open(config_path, 'r') as config_file:
                config = json.load(config_file)
                cls.base_url = config.get('OLLAMA_API_URL', cls.base_url)
            print(f"Loaded Ollama API URL: {cls.base_url}")
        except FileNotFoundError:
            print(f"Warning: config.json not found at {config_path}. Using default URL: {cls.base_url}")
        except json.JSONDecodeError:
            print(f"Error: Invalid JSON in config.json at {config_path}. Using default URL: {cls.base_url}")

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model": (cls.available_models,) if cls.available_models else (["No models found"],),
                "extra_model": ("STRING", {
                    "multiline": False,
                    "default": "none"
                }),
                "theme": ("STRING", {"multiline": True}),
                "max_tokens": ("INT", {"default": 1000, "min": 1, "max": 32768}),
                "temperature": ("FLOAT", {"default": 0.7, "min": 0, "max": 2, "step": 0.1}),
                "prompt_type": (["sdxl", "kolors"],),
                "debug": (["enable", "disable"],),
            },
        }

    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("positive_prompt", "negative_prompt")
    FUNCTION = "generate_sd_prompt"
    CATEGORY = "ğŸŒ™DW/prompt_utils"

    def generate_sd_prompt(self, model, extra_model, theme, max_tokens, temperature, prompt_type, debug):
        if extra_model != "none":
            model = extra_model

        if prompt_type == "sdxl":
            system_message = """ä½ æ˜¯ä¸€ä½æœ‰è‰ºæœ¯æ°”æ¯çš„Stable Diffusion prompt åŠ©ç†ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç»™å®šçš„ä¸»é¢˜ç”Ÿæˆé«˜è´¨é‡çš„Stable Diffusionæç¤ºè¯ã€‚è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹è¦æ±‚ï¼š

1. è¾“å‡ºæ ¼å¼ï¼š
   - ä»¥"Prompt:"å¼€å¤´çš„æ­£é¢æç¤ºè¯
   - ä»¥"Negative Prompt:"å¼€å¤´çš„è´Ÿé¢æç¤ºè¯

2. Promptè¦æ±‚ï¼š
   - å¼€å¤´å¿…é¡»åŒ…å«"(best quality,4k,8k,highres,masterpiece:1.2),ultra-detailed,(realistic,photorealistic,photo-realistic:1.37)"
   - åŒ…å«ç”»é¢ä¸»ä½“ã€æè´¨ã€é™„åŠ ç»†èŠ‚ã€å›¾åƒè´¨é‡ã€è‰ºæœ¯é£æ ¼ã€è‰²å½©è‰²è°ƒã€ç¯å…‰ç­‰
   - å¯¹äºäººç‰©ä¸»é¢˜ï¼Œå¿…é¡»æè¿°çœ¼ç›ã€é¼»å­ã€å˜´å”‡
   - ä½¿ç”¨è‹±æ–‡åŠè§’é€—å·åˆ†éš”
   - ä¸è¶…è¿‡40ä¸ªæ ‡ç­¾ï¼Œ60ä¸ªå•è¯
   - æŒ‰é‡è¦æ€§æ’åº

3. Negative Promptè¦æ±‚ï¼š
   - å¿…é¡»åŒ…å«"nsfw,(low quality,normal quality,worst quality,jpeg artifacts),cropped,monochrome,lowres,low saturation,((watermark)),(white letters)"
   - å¦‚æœæ˜¯äººç‰©ä¸»é¢˜ï¼Œè¿˜è¦åŒ…å«"skin spots,acnes,skin blemishes,age spot,mutated hands,mutated fingers,deformed,bad anatomy,disfigured,poorly drawn face,extra limb,ugly,poorly drawn hands,missing limb,floating limbs,disconnected limbs,out of focus,long neck,long body,extra fingers,fewer fingers,,(multi nipples),bad hands,signature,username,bad feet,blurry,bad body"

è¯·ç›´æ¥ç”Ÿæˆpromptï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–é¢å¤–æ–‡å­—ã€‚åªä½¿ç”¨è‹±æ–‡ï¼Œå³ä½¿ä¸»é¢˜æ˜¯ä¸­æ–‡ã€‚"""
        else:  # prompt_type == "kolors"
            system_message = """ä½ æ˜¯ä¸€ä½ç†Ÿç»ƒçš„AIè‰ºæœ¯ç”Ÿæˆæ¨¡å‹kolorsçš„æç¤ºå·¥ç¨‹å¸ˆï¼Œç±»ä¼¼äºDALLE-3ã€‚ä½ å¯¹æç¤ºè¯çš„å¤æ‚æ€§æœ‰æ·±å…¥çš„ç†è§£ï¼Œç¡®ä¿ç”Ÿæˆçš„è‰ºæœ¯ä½œå“ç¬¦åˆç”¨æˆ·çš„æœŸæœ›ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç»™å®šçš„ä¸»é¢˜ç”Ÿæˆé«˜è´¨é‡çš„kolorsæç¤ºè¯ã€‚è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹è¦æ±‚ï¼š

1. è¾“å‡ºæ ¼å¼ï¼š
   - ç›´æ¥è¾“å‡ºä¸­æ–‡æç¤ºè¯ï¼Œä¸éœ€è¦ä»»ä½•æ ‡é¢˜æˆ–å‰ç¼€

2. æç¤ºè¯è¦æ±‚ï¼š
   - ä½¿ç”¨ä¸­æ–‡è‡ªç„¶è¯­è¨€æè¿°ï¼Œæ˜ç¡®ï¼‹ç²¾ç®€
   - å…ˆæŠŠæœ€éš¾ç”Ÿæˆçš„éƒ¨åˆ†å†™åœ¨å‰é¢ï¼ˆè€Œéä¸»è§’ï¼‰ï¼Œç„¶åå†™å¿…è¦çš„å…ƒç´ å’Œç»†èŠ‚ï¼Œæ¥ç€æ˜¯èƒŒæ™¯ï¼Œç„¶åæ˜¯é£æ ¼ã€é¢œè‰²ç­‰
   - åŠ å…¥ç”»é¢åœºæ™¯ç»†èŠ‚æˆ–äººç‰©ç»†èŠ‚ï¼Œè®©å›¾åƒçœ‹èµ·æ¥æ›´å……å®å’Œåˆç†
   - ç¡®ä¿å†…å®¹ä¸ä¸»é¢˜ç›¸ç¬¦ï¼Œç”»é¢æ•´ä½“å’Œè°

è¯·ç›´æ¥ç”Ÿæˆpromptï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–é¢å¤–æ–‡å­—ã€‚ä¸è¦åŒ…å«è´Ÿå‘æç¤ºè¯ã€‚"""

        prompt = f"æ ¹æ®ä»¥ä¸‹ä¸»é¢˜ç”Ÿæˆ{'Stable Diffusion' if prompt_type == 'sdxl' else 'kolors'}æç¤ºè¯ï¼š{theme}"

        if debug == "enable":
            print(f"Attempting to connect to Ollama at: {self.base_url}")
            print(f"Using model: {model}")
            print(f"Prompt: {prompt}")

        try:
            response = requests.post(f"{self.base_url}/api/generate", json={
                "model": model,
                "prompt": f"{system_message}\n\nHuman: {prompt}\n\nAssistant:",
                "stream": False,
                "max_tokens": max_tokens,
                "temperature": temperature
            })
            response.raise_for_status()
            result = response.json()
            generated_text = result['response']

            if debug == "enable":
                print(f"Generated text: {generated_text}")

            # åˆ†å‰²æ­£é¢å’Œè´Ÿé¢æç¤ºè¯
            if prompt_type == "sdxl":
                if "Prompt:" in generated_text and "Negative Prompt:" in generated_text:
                    parts = generated_text.split("Negative Prompt:")
                    positive_prompt = parts[0].replace("Prompt:", "").strip()
                    negative_prompt = parts[1].strip()
                else:
                    positive_prompt = generated_text
                    negative_prompt = ""
            else:  # prompt_type == "kolors"
                positive_prompt = generated_text.strip()
                negative_prompt = "ä½è´¨é‡ï¼Œåæ‰‹ï¼Œæ°´å°"
            
            return (positive_prompt, negative_prompt)
        except requests.RequestException as e:
            error_message = f"Error: {str(e)}"
            if debug == "enable":
                print(error_message)
            return (error_message, "")

OllamaPromptExtractor.initialize()

NODE_CLASS_MAPPINGS = {
    "OllamaPromptExtractor": OllamaPromptExtractor
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "OllamaPromptExtractor": "Ollama Prompt Extractor"
}