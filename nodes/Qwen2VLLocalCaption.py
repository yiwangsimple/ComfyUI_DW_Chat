import os
import torch
import numpy as np
from PIL import Image
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
import folder_paths
import gc
from qwen_vl_utils import process_vision_info
import cv2

class Qwen2VLLocalCaption:
    def __init__(self):
        self.model = None
        self.processor = None
        self.device = None
        self.precision = None
        print(f"ComfyUI models directory: {folder_paths.models_dir}")
        self.model_path = self.get_model_path()
        print(f"Qwen2-VL model path: {self.model_path}")

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": ("IMAGE",),
                "prompt": ("STRING", {"default": "åˆ†æè¿™å¼ å›¾ç‰‡å¹¶æä¾›è¯¦ç»†æè¿°ã€‚", "multiline": True}),
                "task": (["general", "ocr", "visual_reasoning", "chinese_understanding", "prompt_generation"], {"default": "general"}),
                "temperature": ("FLOAT", {"default": 0.7, "min": 0.0, "max": 1.0, "step": 0.1}),
                "max_tokens": ("INT", {"default": 1024, "min": 1, "max": 2048}),
                "device": (["cuda", "cpu"], {"default": "cuda"}),
                "precision": (["float32", "float16"], {"default": "float16"}),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("result",)
    FUNCTION = "process_image"
    CATEGORY = "ğŸŒ™DW/Qwen2VL"

    def process_image(self, image, prompt, task, temperature, max_tokens, device, precision):
        try:
            if self.model is None or self.device != device or self.precision != precision:
                self.device = device
                self.precision = precision
                self.load_model()

            print(f"Input image type: {type(image)}")
            if isinstance(image, torch.Tensor):
                print(f"Image tensor shape: {image.shape}")
                
                # å¤„ç†ç‰¹æ®Šçš„å›¾åƒå½¢çŠ¶
                if image.shape == (1, 1, 1152):
                    # å‡è®¾è¿™æ˜¯ä¸€ä¸ªè¢«é”™è¯¯è§£é‡Šçš„ RGB å›¾åƒ
                    image = image.squeeze().reshape(16, 24, 3)  # 16 * 24 * 3 = 1152
                elif image.shape[0] == 1:
                    image = image.squeeze(0)
                
                # ç¡®ä¿å›¾åƒæ˜¯ 3 é€šé“çš„
                if image.shape[2] != 3:
                    if image.shape[2] == 1:
                        image = image.repeat(1, 1, 3)
                    else:
                        raise ValueError(f"Unexpected number of channels: {image.shape[2]}")
                
                # è½¬æ¢ä¸º numpy æ•°ç»„
                image = (image.cpu().numpy() * 255).astype(np.uint8)
                
                # è°ƒæ•´å›¾åƒå¤§å°ä¸ºæ¨¡å‹æœŸæœ›çš„å°ºå¯¸ï¼ˆå‡è®¾ä¸º224x224ï¼‰
                image = cv2.resize(image, (224, 224))
                
                # è½¬æ¢ä¸º PIL Image
                image = Image.fromarray(image)
            
            print(f"Processed image size: {image.size}, mode: {image.mode}")

            task_prompts = {
                "general": "åˆ†æè¿™å¼ å›¾ç‰‡å¹¶æä¾›è¯¦ç»†æè¿°ã€‚",
                "ocr": "è¯†åˆ«å¹¶æå–å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—ã€‚",
                "visual_reasoning": "åˆ†æå›¾ç‰‡å¹¶å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š",
                "chinese_understanding": "åˆ†æå›¾ç‰‡å¹¶ç”¨æµç•…çš„ä¸­æ–‡æè¿°å†…å®¹ã€‚",
                "prompt_generation": "åˆ†æè¿™å¼ å›¾ç‰‡å¹¶ç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„æ–‡æœ¬åˆ°å›¾åƒæç¤ºã€‚ä¸è¦åŠ å‰ç¼€ï¼"
            }
            
            full_prompt = f"{task_prompts[task]} {prompt}"
            
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": image},
                        {"type": "text", "text": full_prompt}
                    ]
                }
            ]

            text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            image_inputs, _ = process_vision_info(messages)
            inputs = self.processor(text=[text], images=image_inputs, return_tensors="pt").to(self.device)

            with torch.no_grad():
                generated_ids = self.model.generate(**inputs, max_new_tokens=max_tokens, temperature=temperature, do_sample=True)
            generated_text = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
            # æå–åŠ©æ‰‹çš„å›ç­”
            assistant_response = generated_text.split('assistant\n')[-1].strip()
            
            del inputs, generated_ids
            torch.cuda.empty_cache() if self.device == "cuda" else None
            gc.collect()
            
            return (assistant_response,)
        except Exception as e:
            import traceback
            error_msg = f"é”™è¯¯: {str(e)}\n{traceback.format_exc()}"
            print(error_msg)
            return (error_msg,)

    def load_model(self):
        if self.model_path is None or not os.path.exists(self.model_path):
            raise RuntimeError(f"Model path is invalid or does not exist: {self.model_path}")

        try:
            print(f"Loading model from {self.model_path}")
            self.processor = AutoProcessor.from_pretrained(self.model_path, trust_remote_code=True)
            self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                device_map=self.device,
                torch_dtype=torch.float16 if self.precision == "float16" else torch.float32,
            )
            print("Model loaded successfully")
            
            self.model.to(self.device)
            
        except Exception as e:
            print(f"Error loading model: {e}")
            raise RuntimeError(f"Failed to load the model from {self.model_path}. Error: {str(e)}")

    def get_model_path(self):
        possible_paths = [
            os.path.join(folder_paths.models_dir, "prompt_generator", "Qwen2-VL-2B-Instruct"),
            "models/prompt_generator/Qwen2-VL-2B-Instruct",
            "Qwen2-VL-2B-Instruct",
        ]
        
        for path in possible_paths:
            if os.path.exists(path) and any(os.scandir(path)):
                return path
        
        raise RuntimeError("Could not find the Qwen2-VL model. Please ensure it's placed in one of the expected directories.")

    def unload_model(self):
        del self.model
        del self.processor
        self.model = None
        self.processor = None
        torch.cuda.empty_cache() if self.device == "cuda" else None
        gc.collect()

NODE_CLASS_MAPPINGS = {
    "Qwen2VLLocalCaption": Qwen2VLLocalCaption
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "Qwen2VLLocalCaption": "é€šä¹‰åƒé—®VL æœ¬åœ°å¤šåŠŸèƒ½è§†è§‰åˆ†æ"
}