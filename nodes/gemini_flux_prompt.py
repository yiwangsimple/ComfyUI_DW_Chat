import os
import sys
import google.generativeai as genai
from PIL import Image
import torch
from contextlib import contextmanager
from tenacity import retry, stop_after_attempt, wait_exponential

# æ·»åŠ çˆ¶ç›®å½•åˆ° Python è·¯å¾„
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if parent_dir not in sys.path:
    sys.path.append(parent_dir)

from api_utils import load_api_key

@contextmanager
def temporary_env_var(key: str, new_value):
    old_value = os.environ.get(key)
    if new_value is not None:
        os.environ[key] = new_value
    elif key in os.environ:
        del os.environ[key]
    try:
        yield
    finally:
        if old_value is not None:
            os.environ[key] = old_value
        elif key in os.environ:
            del os.environ[key]

class GeminiFluxPrompt:
    def __init__(self):
        self.client = None
        self.load_api_key()

    def load_api_key(self):
        api_key = load_api_key('GEMINI_API_KEY')
        if api_key:
            genai.configure(api_key=api_key, transport='rest')
            self.client = genai
        else:
            print("é”™è¯¯ï¼šåœ¨ api_key.ini ä¸­æœªæ‰¾åˆ° GEMINI_API_KEY")

    @staticmethod
    def tensor_to_image(tensor):
        tensor = tensor.cpu()
        image_np = tensor.squeeze().mul(255).clamp(0, 255).byte().numpy()
        return Image.fromarray(image_np, mode='RGB')

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    def call_api(self, model, prompt, image=None):
        content = [prompt, image] if image else prompt
        response = model.generate_content(content)
        return response.text

    def generate_prompt(self, text_input, image_input=None):
        if not self.client:
            return "é”™è¯¯ï¼šGEMINI_API_KEY æœªè®¾ç½®æˆ–æ— æ•ˆã€‚è¯·æ£€æŸ¥æ‚¨çš„ api_key.ini æ–‡ä»¶ã€‚", ""

        model = self.client.GenerativeModel('gemini-1.5-pro')

        system_prompt = """ä½ æ˜¯ä¸€ä½æœ‰è‰ºæœ¯æ°”æ¯çš„Stable Diffusion prompt åŠ©ç†ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç»™å®šçš„ä¸»é¢˜ç”Ÿæˆä¸€ä»½è¯¦ç»†çš„ã€é«˜è´¨é‡çš„promptï¼Œè®©Stable Diffusionå¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒã€‚promptå¿…é¡»åŒ…å«"clip-L:"å’Œ"clip-T5:"ä¸¤éƒ¨åˆ†ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

clip-L: [è‹±æ–‡å…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”]

clip-T5: [è‹±æ–‡è‡ªç„¶è¯­è¨€æè¿°]

æ³¨æ„ï¼š
1. clip-L éƒ¨åˆ†åªåŒ…å«å…³é”®è¯æˆ–çŸ­è¯­ï¼Œä¸è¦æœ‰å®Œæ•´å¥å­æˆ–è§£é‡Šã€‚
2. clip-T5 éƒ¨åˆ†ä½¿ç”¨è‡ªç„¶è¯­è¨€æè¿°ã€‚
3. ä¸¤ä¸ªéƒ¨åˆ†éƒ½å¿…é¡»æ˜¯è‹±æ–‡ã€‚
4. ç¡®ä¿æè¿°äººç‰©æ—¶åŒ…å«é¢éƒ¨ç»†èŠ‚ï¼Œå¦‚"beautiful detailed eyes, beautiful detailed lips, extremely detailed eyes and face, long eyelashes"ã€‚
5. æ·»åŠ è‡³å°‘5ä¸ªåˆç†çš„ç”»é¢ç»†èŠ‚ã€‚
6. åŒ…å«æè´¨ã€è‰ºæœ¯é£æ ¼ã€è‰²å½©è‰²è°ƒå’Œç¯å…‰æ•ˆæœçš„æè¿°ã€‚
7. å¿…é¡»åŒæ—¶ç”Ÿæˆ clip-L å’Œ clip-T5 ä¸¤éƒ¨åˆ†å†…å®¹ã€‚
"""

        if image_input is not None:
            pil_image = self.tensor_to_image(image_input)
            if text_input:
                user_prompt = f"è¯·åˆ†æè¿™å¼ å›¾ç‰‡ï¼Œå¹¶ç»“åˆä»¥ä¸‹æ–‡æœ¬ç”ŸæˆStable Diffusion promptã€‚æ–‡æœ¬ï¼š{text_input}"
            else:
                user_prompt = "è¯·åˆ†æè¿™å¼ å›¾ç‰‡ï¼Œå¹¶ç”Ÿæˆç›¸åº”çš„Stable Diffusion promptã€‚"
        else:
            pil_image = None
            user_prompt = f"è¯·æ ¹æ®ä»¥ä¸‹ä¸»é¢˜ç”ŸæˆStable Diffusion promptï¼š{text_input}"

        try:
            with temporary_env_var('HTTP_PROXY', None), temporary_env_var('HTTPS_PROXY', None):
                full_prompt = f"{system_prompt}\n\n{user_prompt}"
                response = self.call_api(model, full_prompt, image=pil_image)
            
            # åˆ†ç¦» clip-L å’Œ clip-T5
            clip_l = ""
            clip_t5 = ""
            current_section = ""
            for line in response.split('\n'):
                if line.startswith("clip-L:"):
                    current_section = "clip-L"
                    clip_l = line.replace("clip-L:", "").strip()
                elif line.startswith("clip-T5:"):
                    current_section = "clip-T5"
                    clip_t5 = line.replace("clip-T5:", "").strip()
                elif current_section == "clip-T5":
                    clip_t5 += " " + line.strip()

            if not clip_l or not clip_t5:
                raise ValueError("API æœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„ clip-L å’Œ clip-T5 å†…å®¹")

            return clip_l, clip_t5
        except Exception as e:
            return f"é”™è¯¯: {str(e)}", f"é”™è¯¯: {str(e)}"

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "text_input": ("STRING", {"default": "", "multiline": True}),
            },
            "optional": {
                "image_input": ("IMAGE",),
            }
        }

    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("clip_L", "clip_T5")
    FUNCTION = "generate"
    CATEGORY = "ğŸŒ™DW/Gemini1.5"

    def generate(self, text_input, image_input=None):
        return self.generate_prompt(text_input, image_input)

NODE_CLASS_MAPPINGS = {
    "GeminiFluxPrompt": GeminiFluxPrompt
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "GeminiFluxPrompt": "Gemini Flux Prompt"
}