import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from PIL import Image
from pathlib import Path
import folder_paths
import os
import shutil
import re
from torchvision.transforms import ToPILImage

# å®šä¹‰æ¨¡å‹å­˜å‚¨ç›®å½•
models_dir = Path(folder_paths.base_path) / "models"
llava_checkpoints_dir = models_dir / "LLavacheckpoints"
files_for_moondream2 = llava_checkpoints_dir / "files_for_moondream2"
files_for_moondream2.mkdir(parents=True, exist_ok=True)

class Moondream2Predictor:
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(Moondream2Predictor, cls).__new__(cls)
            cls._instance.model = None
            cls._instance.tokenizer = None
        return cls._instance

    def load_model(self):
        if self.model is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
            print("åŠ è½½æ¨¡å‹ä¸­...")
            self.model = AutoModelForCausalLM.from_pretrained(files_for_moondream2, trust_remote_code=True).to(self.device)
            self.tokenizer = AutoTokenizer.from_pretrained(files_for_moondream2)
            print("æ¨¡å‹åŠ è½½æˆåŠŸ")

    def generate_predictions(self, image_path, question):
        self.load_model()  # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
        try:
            image_input = Image.open(image_path).convert("RGB")
            enc_image = self.model.encode_image(image_input)
            generated_text = self.model.answer_question(enc_image, question, self.tokenizer)
            return generated_text
        finally:
            if os.path.exists(image_path):
                os.remove(image_path)

    def clear_memory(self):
        if self.model:
            del self.model
            del self.tokenizer
            self.model = None
            self.tokenizer = None
            torch.cuda.empty_cache()
        print("æ¨¡å‹å·²å¸è½½ï¼Œå†…å­˜å·²æ¸…ç†")

class Moondream2model:
    def __init__(self):
        self.predictor = Moondream2Predictor()

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": ("IMAGE",),
                "text_input": ("STRING", {"multiline": True, "default": ""}),
            },
        }

    RETURN_TYPES = ("STRING",)
    FUNCTION = "moondream2_generate_predictions"
    CATEGORY = "ğŸŒ™DW/dwimage2"

    def moondream2_generate_predictions(self, image, text_input):
        try:
            pil_image = ToPILImage()(image[0].permute(2, 0, 1))
            temp_path = files_for_moondream2 / "temp_image.png"
            pil_image.save(temp_path)
            
            response = self.predictor.generate_predictions(temp_path, text_input)
            response = ' '.join(response.strip().split())
           
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç§»é™¤å¼€å¤´çš„ç‰¹å®šçŸ­è¯­
            response = re.sub(r'^(The image contains|The image contains|This picture contains|This picture contains|The image contains|The image contains|This image contains|This image contains)\s*', '', response, flags=re.IGNORECASE)
            
            return (response,)
        finally:
            # ç¡®ä¿åœ¨å‡½æ•°ç»“æŸæ—¶å¸è½½æ¨¡å‹
            self.predictor.clear_memory()

NODE_CLASS_MAPPINGS = {
    "dwimage2": Moondream2model
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "dwimage2": "DW Image2 Chat"
}